% Developer's Guide

Style
=====

We have provided an Eclipse Formatter file `formatter.xml` with all the
formatting conventions currently used in the project.  Highlights include no
tabs, 4-space indentation, and 120-char width.  Please respect this so as to
reduce the amount of formatting-related noise produced in commits.

Static Analysis
===============

We would like to use static analysis tools PMD and FindBugs to maintain code
quality.  However, we have not yet arrived at a consensus on what rules to
adhere to, and what to ignore.

Netty Notes
===========

The asynchronous network IO infrastructure that Hedwig uses is [Netty].  Here
are some notes on Netty's concurrency architecture and its filter pipeline
design.

[Netty]: http://www.jboss.org/netty

Concurrency Architecture
------------------------

After calling `ServerBootstrap.bind()`, Netty starts a boss thread
(`NioServerSocketPipelineSink.Boss`) that just accepts new connections and
registers them with one of the workers from the `NioWorker` pool in round-robin
fashion (pool size defaults to CPU count).  Each worker runs its own select
loop over just the set of keys that have been registered with it.  Workers
start lazily on demand and run only so long as there are interested fd's/keys.
All selected events are handled in the same thread and sent up the pipeline
attached to the channel (this association is established by the boss as soon as
a new connection is accepted).

All workers, and the boss, run via the executor thread pool; hence, the
executor must support at least two simultaneous threads.

Handler Pipeline
----------------

A pipeline implements the intercepting filter pattern.  A pipeline is a
sequence of handlers.  Whenever a packet is read from the wire, it travels up
the stream, stopping at each handler that can handle upstream events.
Vice-versa for writes.  Between each filter, control flows back through the
centralized pipeline, and a linked list of contexts keeps track of where we are
in the pipeline (one context object per handler).

Distributed Performance Evaluation
==================================

We've included some scripts to repeatedly run varying configurations of Hedwig
on a distributed testbed and collect the resulting data.  The experiments use
the `org.apache.hedwig.client.App` client application and are driven by
`scripts/hw.bash` (via the `app` command).

Currently, we have two types of experiments: subscription benchmarks and
publishing benchmarks.

Subscription Benchmarks
-----------------------

The subscription benchmark compares synchronous and asynchronous subscriptions.
Because the synchronicity of subscriptions is a server configuration parameter,
the servers must be restarted to change this.  The benchmarks varies the
maximum number of outstanding subscription requests.

To run the subscription benchmark with wilbur6 as the subscriber and wilbur1 as
its default hub:

  hosts=wilbur6 scripts/hw.bash sub-exp wilbur1

This produces log files into the `sub` directory, which may then be analyzed
using the analysis scripts.

Publishing Benchmarks
---------------------

The publishing benchmark measures the throughput and latency of publishing
messages within a LAN and across a WAN.  It varies the following parameters:

- maximum number of outstanding publish requests
- number of publishers
- number of (local) receivers

We vary each dimension separately (and have default settings) to avoid a
combinatorial explosion in the number of configurations to explore.

First, start a (standalone) instance:

  scripts/hw.bash start-region '' $hwhost $zkhost $bk1host $bk2host $bk3host

To run this over `$host1` through `$host3`, with the number of
publishers/subscribers varying linearly over this set:

  npars="20 40 60 80 100" scripts/hw.bash pub-exps "$host1 $host2 $host3" $hwhost $zkhost

This will vary the number of outstanding publish requests as specified in
`npars`.

You may also optionally run this experiment with a second subscribing region:

  scripts/hw.bash start-zk-bks $zkhost $bk1host $bk2host $bk3host
  npars="..." scripts/hw.bash pub-exps "$host1 $host2 $host3" $hwhost $zkhost $rrecv $rhwhost $rzkhost

where the final three extra arguments specify the client receiver, Hedwig, and
Zookeeper hosts, in that order.

This command will produce files into `./pub/`, which can then be process using
`analyze.py`.

Analysis and Visualization
==========================

`scripts/analyze.py` produces plots from the collected experimental data.  It
has just a few immediate dependencies. In the following, the
indentation signifies nested dependencies, like an upside-down tree:

      component AAA that component AA requires
      component AAB that component AA requires
    component AA that component A requires
      component ABA that component AB requires
      component ABB that component AB requires
    component AB that component A requires
  component A that analysis tools depend on
      component BAA that component BA requires
      component BAB that component BA requires
    component BA that component B requires
      component BBA that component BB requires
      component BBB that component BB requires
    component BB that component B requires
  component B that analysis tools depend on

The reason the tree is upside-down is so that you can treat this whole thing as
a chunk of bash script.

[toast] is a utility that makes it a breeze to install all this software, but
you do need to make sure your environment is set up correctly (e.g.
`PKG_CONFIG_PATH` must point to `~/.toast/armed/lib/pkgconfig/`).

Setup:

  wget -O- http://toastball.net/toast/toast|perl -x - arm toast

  toast arm "http://www.python.org/ftp/python/2.6.2/Python-2.6.2.tar.bz2"

  toast arm numpy

        toast arm libpng

        toast arm pixman

        toast arm freetype

          toast arm 'ftp://xmlsoft.org/libxml2/libxml2-2.7.3.tar.gz'

        toast arm fontconfig

      toast arm cairo

    toast arm pycairo

  hg clone https://yang@bitbucket.org/yang/pycha/
  pycha/setup.bash -d -p $path_to_install_to

  svn co https://assorted.svn.sourceforge.net/svnroot/assorted/python-commons/trunk/ python-commons/
  python-commons/setup.bash -d -p $path_to_install_to

To analyze the publishing experiments, change to the `pub` data directory and
run:

  scripts/analyze.py pub

To analyze the subscription experiments, change to the `sub` data directory
and run:

  scripts/analyze.py sub

[toast]: http://toastball.net/toast/

Debugging
=========

You can attach an Eclipse debugger (or any debugger) to a Java process running
on a remote host, as long as it has been started with the appropriate JVM
flags.  (See the Building Hedwig document to set up your Eclipse environment.)
To launch something using `hw.bash` with debugger attachment enabled, prefix
the command with `attach=true`, e.g.:

  attach=true scripts/hw.bash start-regions myregions.cfg

Profiling
=========

The scripts we have provided include ways for you to launch with YourKit
profiling enabled.

To deploy YourKit onto a number of machines:

  hosts="..." scripts/hw.bash setup-yjp $path_to_yjp

where the path points to the [YourKit Linux zip archive] (which is freely
available and doesn't require any license to use).

Now when using the scripts to run distributed experiments, to profile anything
with YourKit, prefix the command with `use_yjp=true`.  E.g.:

  use_yjp=true scripts/hw.bash start-regions regions.cfg

Now you may start on your local machine the YourKit GUI and connect to the
hosts that you're interested in.

Note that you may want to disable the default set of filters in YourKit.

[YourKit Linux zip archive]: http://www.yourkit.com/download/yjp-8.0.15.zip

Pseudocode
==========

This summarizes the control flow through the system.

  publishhandler
    topicmgr.getowner
      (maybe) claim the topic, calling back into persmgr.acquiredtopic
        read /hedwig/standalone/topics/TOPIC (which should initially be empty)
        for each line, parse as "STARTSEQ\tLEDGERID" # TODO how is this written?
          ledger = bk.openledger(ledgerid)
          lastid = ledger.getlast
          if lastid > 0, lrs[startseq] = persmgr.ledger2lr[ledgerid] = new LedgerRange(ledger, ledgerid, startseq, startseq + lastid # TODO what are ledger ranges?
        create new ledger for topic
          # TODO read
          lr = new LedgerRange(ledger, ledgerid, lastid, -1)
          lrs[lastid] = lr
          persmgr.topic2ranges[topic] = lrs
    add region info to pub req and send that to persmgr.persistmessage
      entryid = persmgr.topic2ranges[topic].last.ledger.addentry(the pub'd data)
      update persmgr.topic2lastseq[topic]:
        .local = persmgr.ledger2lr[ledger id].startseq + entryid
        .regions = maxes of orig seq and incoming pub seq

  subscribehandler
    topicmgr.getowner...
    delivmgr.startservingsubscription(topic, endpoint, ishubsubscriber)
      delivmgr.endpoint2sub[endpoint] = new subscriber(lastseq = persmgr.getcurrentseqidfortopic(topic).local)
      delivmgr.topic2ptr2subs[topic][ptr].add(sub)
      sub.delivernextmessage
        sub.curseq = persmgr.getseqidafterskipping(topic, sub.lastseq, skip = 1)
        msg = persmgr.scansinglemessage(topic, seq = sub.curseq)
          if persmgr.topic2lastseq[topic].local >= seq
            lr = persmgr.topic2ranges[topic].floor(seq)
            return lr.ledger.read(first = last = seq - lr.startseq)
        if failed, then retry in 1 s
        endpoint.send(msg)
        movedeliveryptr
          delivmgr.topic2ptr2subs[topic][sub.lastseq].remove(sub)
          delivmgr.topic2ptr2subs[topic][sub.curseq].add(sub)
        previd = sub.lastseq, sub.lastseq = sub.curseq
        sub.delivernextmessage...

ReadAhead Cache
================

The delivery manager class is responsible for pushing published messages from 
the hubs to the subscribers. The most common case is that all subscribers are 
connected and either caught up, or close to the tail end of the topic. In this 
case, we don't want the delivery manager to be polling bookkeeper for any newly 
arrived messages on the topic; new messages should just be pushed to the 
delivery manager. However, there is also the uncommon case when a subscriber is 
behind, and messages must be pulled from Bookkeeper.

Since all publishes go through the hub, it is possible to cache the recently 
published messages in the hub, and then the delivery manager won't have to make 
the trip to bookkeeper to get the messages but instead get them from local 
process memory.

These ideas of push, pull, and caching are unified in the following way:
- A hub has a cache of messages

- When the delivery manager wants to deliver a message, it asks the cache for 
  it. There are 3 cases:
  - The message is available in the cache, in which case it is given to the 
    delivery manager
  - The message is not present in the cache and the seq-id of the message is 
    beyond the last message published on that topic (this happens if the 
    subscriber is totally caught up for that topic). In this case, a stub is put 
    in the cache in order to notify the delivery manager when that message does 
    happen to be published.
  - The message is not in the cache but has been published to the topic. In this 
    case, a stub is put in the cache, and a read is issued to bookkeeper.

- Whenever a message is published, it is cached. If there is a stub already in 
  the cache for that message, the delivery manager is notified. 

- Whenever a message is read from bookkeeper, it is cached. There must be a stub 
  for that message (since reads to bookkeeper are issued only after putting a 
  stub), so the delivery manager is notified. 

- The cache does readahead, i.e., if a message requested by the delivery manager 
  is not in the cache, a stub is established not only for that message, but also 
  for the next n messages where n is configurable (default 10). On a cache hit, 
  we look ahead n/2 messages, and if that message is not present, we establish 
  another n/2 stubs. In short, we always ensure that the next n stubs are always 
  established.

- Over time, the cache will grow in size. There are 2 pruning mechanisms:
  
  - Once all subscribers have consumed up to a particular seq-id, they notify 
    the cache, and all messages up to that seq-id are pruned from the cache.
  - If the above pruning is not working (e.g., because some subscribers are 
    down), the cache will eventually hit its size limit which is configurable  
    (default, half of maximum jvm heap size). At this point, messages are just 
    pruned in FIFO order. We use the size of the blobs in the message for 
    estimating the cache size. The assumption is that that size will dominate 
    over fixed, object-level size overheads. 
  - Stubs are not purged because according to the above simplification, they are 
    of 0 size.

Scalability Bottlenecks Down the Road
=====================================

- Currently each topic subscription is served on a different channel. The number 
  of channels will become a bottleneck at higher channels. We should switch to 
  an architecture, where multiple topic subscriptions between the same client, 
  hub pair should be served on the same channel. We can have commands to start, 
  stop subscriptions sent all the way to the server (right now these are local).
- Publishes for a topic are serialized through a hub, to get ordering 
  guarantees. Currently, all subscriptions to that topic are served from the 
  same hub. If we start having large number of subscribers to heavy-volume 
  topics, the outbound bandwidth at the hub, or the CPU at that hub might become 
  the bottleneck. In that case, we can setup other regions through which the 
  messages are routed (this hierarchical scheme) reduces bandwidth requirements 
  at any single node. It should be possible to do this entirely through 
  configuration.

